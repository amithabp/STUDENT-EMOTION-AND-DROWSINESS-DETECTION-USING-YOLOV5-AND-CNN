# -*- coding: utf-8 -*-
"""emotion-classification CNN 5 LAYER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dyLV8PkmGNVZAXzP73Dd8MP0nefzVxsQ

# Data processing
"""

import matplotlib.pyplot as plt
import cv2
import tensorflow as tf
import numpy as np
import pathlib
import datetime

import kaggle


 kaggle datasets download -d ananthu017/emotion-detection-fer

 unzip emotion-detection-fer

data_dir = pathlib.Path("/content/train")
image_count = len(list(data_dir.glob('*/*.png')))
print(image_count)
# classnames in the dataset specified
CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') if item.name != "LICENSE.txt" ])
print(CLASS_NAMES)
# print length of class names
output_class_units = len(CLASS_NAMES)
print(output_class_units)

model = tf.keras.models.Sequential([
    # 1st conv
  tf.keras.layers.Conv2D(96, (11,11),strides=(4,4), activation='relu', input_shape=(227, 227, 3)),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPooling2D(2, strides=(2,2)),
    # 2nd conv
  tf.keras.layers.Conv2D(256, (11,11),strides=(1,1), activation='relu',padding="same"),
  tf.keras.layers.BatchNormalization(),
     # 3rd conv
  tf.keras.layers.Conv2D(384, (3,3),strides=(1,1), activation='relu',padding="same"),
  tf.keras.layers.BatchNormalization(),
    # 4th conv
  tf.keras.layers.Conv2D(384, (3,3),strides=(1,1), activation='relu',padding="same"),
  tf.keras.layers.BatchNormalization(),
    # 5th Conv
  tf.keras.layers.Conv2D(256, (3, 3), strides=(1, 1), activation='relu',padding="same"),
  tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPooling2D(2, strides=(2, 2)),
  # To Flatten layer
  tf.keras.layers.Flatten(),
  # To FC layer 1
  tf.keras.layers.Dense(4096, activation='relu'),
    # add dropout 0.5 ==> tf.keras.layers.Dropout(0.5),
  #To FC layer 2
  tf.keras.layers.Dense(4096, activation='relu'),
    # add dropout 0.5 ==> tf.keras.layers.Dropout(0.5),
  tf.keras.layers.Dense(output_class_units, activation='sigmoid')
])

data_dir2 = pathlib.Path("/content/test")

BATCH_SIZE = 32             # Can be of size 2^n, but not restricted to. for the better utilization of memory
IMG_HEIGHT = 227            # input Shape required by the model
IMG_WIDTH = 227             # input Shape required by the model
STEPS_PER_EPOCH = np.ceil(image_count/BATCH_SIZE)

# Rescalingthe pixel values from 0~255 to 0~1 For RGB Channels of the image.
image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)
# training_data for model training
train_data_gen = image_generator.flow_from_directory(directory=str(data_dir),
                                                     batch_size=BATCH_SIZE,
                                                     shuffle=True,
                                                     target_size=(IMG_HEIGHT, IMG_WIDTH), #Resizing the raw dataset
                                                     classes = list(CLASS_NAMES))

val_data_gen = image_generator.flow_from_directory(directory=str(data_dir),
                                                     batch_size=BATCH_SIZE,
                                                     shuffle=True,
                                                     target_size=(IMG_HEIGHT, IMG_WIDTH), #Resizing the raw dataset
                                                     classes = list(CLASS_NAMES))

test_data_gen = image_generator.flow_from_directory(directory=str(data_dir2),
                                                     batch_size=BATCH_SIZE,
                                                     shuffle=True,
                                                     target_size=(IMG_HEIGHT, IMG_WIDTH), #Resizing the raw dataset
                                                     classes = list(CLASS_NAMES))

model.compile(optimizer='adam', loss="categorical_crossentropy", metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])

# Summarizing the model architecture and printing it out
model.summary()

history_1 = model.fit(train_data_gen,
      steps_per_epoch=STEPS_PER_EPOCH,
      epochs=20,
    validation_data=val_data_gen)

print("Testing Accuracy",model.evaluate(test_data_gen ))

model.save('my_model.h5')

# Accessing the accuracy and loss values from history object
train_acc = history_1.history['accuracy']
val_acc = history_1.history['val_accuracy']
train_loss = history_1.history['loss']
val_loss = history_1.history['val_loss']

# Plotting the train and validation accuracy curves
plt.plot(train_acc, label='Train Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plotting the train and validation loss curves
plt.plot(train_loss, label='Train Loss')
plt.plot(val_loss, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Evaluating the model on test data
y_pred = model.predict(test_data_gen)
y_pred = np.argmax(y_pred, axis=1)
y_true = test_data_gen.labels



print("Testing Accuracy:", model.evaluate(test_data_gen))

# Accessing the accuracy and loss values from history object
train_acc = history_1.history['accuracy']
val_acc = history_1.history['val_accuracy']
train_loss = history_1.history['loss']
val_loss = history_1.history['val_loss']

# Plotting the train and validation accuracy curves
plt.plot(train_acc, label='Train Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plotting the train and validation loss curves
plt.plot(train_loss, label='Train Loss')
plt.plot(val_loss, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Evaluating the model on test data
test_loss, test_acc, test_precision, test_recall = model.evaluate(test_data_gen)
print("Testing Loss:", test_loss)
print("Testing Accuracy:", test_acc)
print("Testing Precision:", test_precision)
print("Testing Recall:", test_recall)

# Creating the confusion matrix
test_labels = test_data_gen.classes
predictions = model.predict(test_data_gen)
cm = tf.math.confusion_matrix(test_labels, np.argmax(predictions, axis=1)).numpy()

# Calculating the true positives, true negatives, false positives, false negatives
tp = np.diag(cm)
tn = []
fp = []
fn = []
for i in range(output_class_units):
    tn.append(np.sum(cm) - np.sum(cm[i,:]) - np.sum(cm[:,i]) + cm[i,i])
    fp.append(np.sum(cm[:,i]) - cm[i,i])
    fn.append(np.sum(cm[i,:]) - cm[i,i])

# Creating the evaluation matrix
precision = tp / (tp + fp)
recall = tp / (tp + fn)
f1_score = 2 * ((precision * recall) / (precision + recall))

# Printing the confusion matrix and evaluation matrix
print("Confusion Matrix:\n", cm)
print("True Positives:", tp)
print("True Negatives:", tn)
print("False Positives:", fp)
print("False Negatives:", fn)
print("Precision:", precision)
print("Recall:", recall)
print("F1-Score:", f1_score)

# Creating the confusion matrix image
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.colorbar()
tick_marks = np.arange(len(CLASS_NAMES))
plt.xticks(tick_marks, CLASS_NAMES, rotation=45)
plt.yticks(tick_marks, CLASS_NAMES)
plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
width, height = cm.shape
for x in range(width):
    for y in range(height):
        plt.annotate(str(cm[x][y]), xy=(y, x), 
                    horizontalalignment='center',
                    verticalalignment='center')
plt.show()

# Creating the grouped bar chart
labels = CLASS_NAMES
x = np.arange(len(labels))
width = 0.2

fig, ax = plt.subplots(figsize=(10, 8))
rects1 = ax.bar(x - width, tp, width, label='True Positive', color='blue')
rects2 = ax.bar(x, tn, width, label='True Negative', color='green')
rects3 = ax.bar(x + width, fp, width, label='False Positive', color='red')
rects4 = ax.bar(x + 2*width, fn, width, label='False Negative', color='orange')

# Adding x-axis labels and legend
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()

# Displaying the plot
plt.show()